{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bd66ee",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook, we will ask you a series of questions regarding model selection. Based on your responses, we will ask you to create the ML models that you've chosen. \n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient third machine learning model in this project, we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "**Note**: Use the dataset that you've created in your previous data transformation step (not the original model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b90a0",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Is this a classification or regression task?  \n",
    "\n",
    "Answer here\n",
    "\n",
    "Classification.\n",
    "\n",
    "Fraud = 1\n",
    "Not Fraud = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bfb9f",
   "metadata": {},
   "source": [
    "Are you predicting for multiple classes or binary classes?  \n",
    "\n",
    "Answer here\n",
    "\n",
    "Binary class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd9378",
   "metadata": {},
   "source": [
    "Given these observations, which 2 (or possibly 3) machine learning models will you choose?  \n",
    "\n",
    "List your models here\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a7aeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "transactions=pd.read_csv(\"../data/bank_transactions_final.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c408b67",
   "metadata": {},
   "source": [
    "## First Model\n",
    "\n",
    "Using the first model that you've chosen, implement the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fab3d0",
   "metadata": {},
   "source": [
    "### 1) Create a train-test split\n",
    "\n",
    "Use your cleaned and transformed dataset to divide your features and labels into training and testing sets. Make sure youâ€™re only using numeric or properly encoded features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c42a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 700000 samples\n",
      "Testing set size: 300000 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select relevant numeric features (including encoded 'type_mapped')\n",
    "features = ['type_mapped', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "X = transactions[features]\n",
    "y = transactions['isFraud']\n",
    "\n",
    "# Split data: 70% train, 30% test (you can adjust test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y  \n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c97f67",
   "metadata": {},
   "source": [
    "### 2) Search for best hyperparameters\n",
    "Use tools like GridSearchCV, RandomizedSearchCV, or model-specific tuning functions to find the best hyperparameters for your first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c8fc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=44; total time=   1.8s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=44; total time=   1.9s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=52; total time=   2.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=52; total time=   2.3s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=50; total time=   2.3s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=65; total time=   2.7s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=65; total time=   2.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=51; total time=   2.8s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=67; total time=   2.9s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=67; total time=   2.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=51; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=62; total time=   2.7s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=71; total time=   2.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=62; total time=   2.8s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=71; total time=   2.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=73; total time=   2.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=3, n_estimators=73; total time=   2.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=76; total time=   2.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=76; total time=   2.2s\n",
      "Best hyperparameters: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 51}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(30, 80),\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': randint(2, 5),\n",
    "    'min_samples_leaf': randint(1, 3),\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Sample 30% of training data (features and labels aligned)\n",
    "sample_frac = 0.3\n",
    "sample_indices = X_train.sample(frac=sample_frac, random_state=42).index\n",
    "X_sample = X_train.loc[sample_indices]\n",
    "y_sample = y_train.loc[sample_indices]\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,          # 10 random combinations\n",
    "    cv=2,               # 2-fold cross-validation\n",
    "    scoring='roc_auc',  # Use ROC AUC for imbalanced classification\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the sampled training data\n",
    "random_search.fit(X_sample, y_sample)\n",
    "\n",
    "# Output best hyperparameters\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Save best model for later use\n",
    "best_rf = random_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30eee3",
   "metadata": {},
   "source": [
    "### 3) Train your model\n",
    "Select the model with best hyperparameters and generate predictions on your test set. Evaluate your models accuracy, precision, recall, and sensitivity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4181c17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9996\n",
      "Precision: 0.9963\n",
      "Recall (Sensitivity):    0.6838\n",
      "F1 Score:  0.8110\n",
      "\n",
      "Confusion Matrix:\n",
      "[[299610      1]\n",
      " [   123    266]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Train best model on the full training data\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)  # Recall is sensitivity here\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity):    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db128d64",
   "metadata": {},
   "source": [
    "## Second Model\n",
    "\n",
    "Create a second machine learning object and rerun steps (2) & (3) on this model. Compare accuracy metrics between these two models. Which handles the class imbalance more effectively?\n",
    "\n",
    "Create as many code-blocks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "732baab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier performance:\n",
      "Accuracy:  0.9995\n",
      "Precision: 0.8466\n",
      "Recall:    0.7943\n",
      "F1 Score:  0.8196\n",
      "\n",
      "Confusion Matrix:\n",
      "[[299555     56]\n",
      " [    80    309]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Initialize Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train on full training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "precision = precision_score(y_test, y_pred_dt)\n",
    "recall = recall_score(y_test, y_pred_dt)\n",
    "f1 = f1_score(y_test, y_pred_dt)\n",
    "conf_mat = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Decision Tree Classifier performance:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66c411",
   "metadata": {},
   "source": [
    "### (Bonus/Optional) Third Model\n",
    "\n",
    "Create a third machine learning model and rerun steps (2) & (3) on this model. Which model has the best predictive capabilities? \n",
    "\n",
    "Create as many code-blocks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3f5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
